# -*- coding: utf-8 -*-
"""DALEEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JIiMUaEo73YVRqili_ML8HXNe9FX7VB4
"""

!pip install fastapi uvicorn nest-asyncio pyngrok transformers scikit-learn

import os
import re
import pickle

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from pyngrok import ngrok

# Load the CSV files
reviews_df = pd.read_csv('/content/user_review_finalversion.csv')
ratings_df = pd.read_csv('/content/user_rating_finalversion.csv')
restaurants_df = pd.read_csv('/content/Updated_Restaurants_Data.csv')

# Display the first few rows of each DataFrame
print("Reviews DataFrame:")
reviews_df.head()

restaurants_df.head()

restaurants_df["simplified_category"].isnull()

print("Ratings DataFrame:")
ratings_df.head()

print("Restaurants DataFrame:")
restaurants_df.head()

# --- Step 1: Normalize column names ---
def normalize_column_names(df):
    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('[^a-z0-9_]', '', regex=True)
    return df

reviews_df = normalize_column_names(reviews_df)
ratings_df = normalize_column_names(ratings_df)
restaurants_df = normalize_column_names(restaurants_df)

# --- Step 2: Check and handle missing values ---
def check_missing_values(df, name):
    missing = df.isnull().sum()
    print(f"\nMissing values in {name}:\n{missing[missing > 0]}")

check_missing_values(reviews_df, "reviews_df")
check_missing_values(ratings_df, "ratings_df")
check_missing_values(restaurants_df, "restaurants_df")

# Fill or drop missing values accordingly (dropping isn't the best practice)
reviews_df.dropna(subset=['review'], inplace=True)
ratings_df.dropna(subset=['userid', 'restid', 'average_rating'], inplace=True) # Changed column names to lowercase and replaced spaces with underscores
restaurants_df.dropna(subset=['restid', 'res_name'], inplace=True) # Changed column names to lowercase and replaced spaces with underscores

# --- Step 3: Remove duplicates ---
reviews_df.drop_duplicates(inplace=True)
ratings_df.drop_duplicates(inplace=True)
restaurants_df.drop_duplicates(inplace=True)

# --- Step 4: Convert data types ---
ratings_df['average_rating'] = pd.to_numeric(ratings_df['average_rating'], errors='coerce')
ratings_df['userid'] = ratings_df['userid'].astype(str) # Changed 'user_id' to 'userid'
reviews_df['userid'] = reviews_df['userid'].astype(str) # Changed 'user_id' to 'userid'
# Access the correct column name after normalization (e.g., 'restaurantid' might have become 'restaid')
reviews_df['restid'] = reviews_df['restid'].astype(str)  # Replace 'restaid' with the actual normalized column name
ratings_df['restid'] = ratings_df['restid'].astype(str)  # Replace 'restaid' with the actual normalized column name
restaurants_df['restid'] = restaurants_df['restid'].astype(str)  # Replace 'restaid' with the actual normalized column name

# --- Step 5 (Updated): Text cleaning for English reviews ---

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

if 'review' in reviews_df.columns:
    reviews_df['cleaned_review'] = reviews_df['review'].apply(clean_text)

# --- Step 6: Merge datasets ---
# Merge reviews with ratings on user_id and restaurant_id
merged_df = pd.merge(ratings_df, reviews_df, on=['userid', 'restid'], how='inner')

# Merge with restaurants data on restaurant_id
final_df = pd.merge(merged_df, restaurants_df, on='restid', how='left')

# Final check
print("\nFinal merged dataset shape:", final_df.shape)
print("Columns in final dataset:", final_df.columns.tolist())

rating_counts = final_df['city_name'].value_counts().sort_index()
print(rating_counts)

# Label Encoding for user and restaurant IDs
user_encoder = LabelEncoder()
restaurant_encoder = LabelEncoder()
ratings_df['original_restid'] = ratings_df['restid']


ratings_df['userid'] = user_encoder.fit_transform(ratings_df['userid'])
ratings_df['restid'] = restaurant_encoder.fit_transform(ratings_df['restid'])

# Verify the transformations
ratings_df.head()

ratings_df.to_csv("ratings_encoded.csv", index=False)
reviews_df.to_csv("reviews_processed.csv", index=False)
restaurants_df.to_csv("restaurants_processed.csv", index=False)

# Save user_encoder
with open("user_encoder.pkl", "wb") as f:
    pickle.dump(user_encoder, f)

# Save restaurant_encoder
with open("restaurant_encoder.pkl", "wb") as f:
    pickle.dump(restaurant_encoder, f)

print("Columns in reviews dataset:", reviews_df.columns.tolist())
print("Columns in resturant dataset:", restaurants_df.columns.tolist())
print("Columns in rating dataset:", ratings_df.columns.tolist())

class NCF(nn.Module):
    def __init__(self, num_users, num_restaurants, embedding_dim, hidden_layer_sizes):
        super(NCF, self).__init__()

        # Embedding layers for user and restaurant IDs
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.restaurant_embedding = nn.Embedding(num_restaurants, embedding_dim)

        # MLP layers
        layers = []
        input_dim = embedding_dim * 2  # Combining both user and restaurant embeddings

        for layer_size in hidden_layer_sizes:
            layers.append(nn.Linear(input_dim, layer_size))
            layers.append(nn.ReLU())
            input_dim = layer_size

        self.mlp = nn.Sequential(*layers)

        # Final prediction layer
        self.output_layer = nn.Linear(input_dim, 1)

    def forward(self, user_id, restaurant_id):
        user_embedding = self.user_embedding(user_id)
        restaurant_embedding = self.restaurant_embedding(restaurant_id)

        # Concatenate embeddings
        x = torch.cat([user_embedding, restaurant_embedding], dim=-1)

        # Pass through MLP
        x = self.mlp(x)

        # Predict the rating
        return self.output_layer(x)

# Convert the data into PyTorch tensors
user_ids = torch.tensor(ratings_df['userid'].values)
restaurant_ids = torch.tensor(ratings_df['restid'].values)
ratings = torch.tensor(ratings_df['average_rating'].values, dtype=torch.float32)

# Create a DataLoader
dataset = TensorDataset(user_ids, restaurant_ids, ratings)
train_loader = DataLoader(dataset, batch_size=64, shuffle=True)

# Initialize the model
num_users = len(user_encoder.classes_)
num_restaurants = len(restaurant_encoder.classes_)
embedding_dim = 50
hidden_layer_sizes = [128, 64]  # Example hidden layers

model = NCF(num_users, num_restaurants, embedding_dim, hidden_layer_sizes)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 50
accuracy_threshold = 0.5  # Example threshold for accuracy

# Lists to store training loss and accuracy for plotting
train_losses = []  # Initialize an empty list to store training losses
accuracies = []  # Initialize an empty list to store accuracies

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    for batch in train_loader:
        user_id, restaurant_id, rating = batch

        # Forward pass
        predicted_ratings = model(user_id, restaurant_id)

        # Compute loss
        loss = criterion(predicted_ratings.squeeze(), rating)

        # Calculate accuracy
        diff = torch.abs(predicted_ratings.squeeze() - rating)
        correct_predictions += (diff < accuracy_threshold).sum().item()
        total_predictions += rating.size(0)  # Number of predictions in the batch

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    # Calculate accuracy for the epoch
    accuracy = correct_predictions / total_predictions * 100

    # Append loss and accuracy for plotting
    train_losses.append(total_loss / len(train_loader))  # Append average loss for the epoch
    accuracies.append(accuracy)  # Append accuracy for the epoch

    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}, Accuracy: {accuracy:.2f}%")



# Calculate RMSE
def calculate_rmse(model, data_loader):
    model.eval()
    total_rmse = 0
    total_samples = 0

    with torch.no_grad():
        for batch in data_loader:
            user_id, restaurant_id, rating = batch
            predicted_ratings = model(user_id, restaurant_id)
            # Calculate MSE for the batch
            mse = nn.MSELoss()(predicted_ratings.squeeze(), rating)
            total_rmse += mse.item() * len(rating)  # Multiply by number of samples in the batch
            total_samples += len(rating)

    rmse = torch.sqrt(torch.tensor(total_rmse / total_samples))
    return rmse.item()

# After training, calculate RMSE on the training data
train_rmse = calculate_rmse(model, train_loader)
print(f"Training RMSE: {train_rmse:.4f}")

train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)

train_user_ids = torch.tensor(train_df['userid'].values)
train_restaurant_ids = torch.tensor(train_df['restid'].values)
train_ratings = torch.tensor(train_df['average_rating'].values, dtype=torch.float32)

test_user_ids = torch.tensor(test_df['userid'].values)
test_restaurant_ids = torch.tensor(test_df['restid'].values)
test_ratings = torch.tensor(test_df['average_rating'].values, dtype=torch.float32)

train_dataset = TensorDataset(train_user_ids, train_restaurant_ids, train_ratings)
test_dataset = TensorDataset(test_user_ids, test_restaurant_ids, test_ratings)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

model.eval()
predicted_ratings_list = []
actual_ratings_list = []

correct_predictions = 0
total_predictions = 0

with torch.no_grad():
    for batch in test_loader:
        user_id, restaurant_id, rating = batch
        predicted_ratings = model(user_id, restaurant_id)
        predicted_ratings_list.extend(predicted_ratings.squeeze().cpu().numpy())
        actual_ratings_list.extend(rating.cpu().numpy())

        diff = torch.abs(predicted_ratings.squeeze() - rating)
        correct_predictions += (diff < accuracy_threshold).sum().item()
        total_predictions += rating.size(0)



mse = mean_squared_error(actual_ratings_list, predicted_ratings_list)
rmse = np.sqrt(mse)
accuracy = correct_predictions / total_predictions * 100

print(f"Test MSE: {mse:.4f}")
print(f"Test RMSE: {rmse:.4f}")
print(f"Test Accuracy: {accuracy:.2f}%")

# Plotting the training loss and accuracy
plt.figure(figsize=(12, 6))

# Plot loss
plt.subplot(1, 2, 1)
plt.plot(range(num_epochs), train_losses, label="Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss vs Epochs")
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(range(num_epochs), accuracies, label="Accuracy", color="green")
plt.xlabel("Epochs")
plt.ylabel("Accuracy (%)")
plt.title("Training Accuracy vs Epochs")
plt.legend()

plt.tight_layout()
plt.show()

torch.save(model, "ncf_full_model.pth")
# Save only the model weights
torch.save(model.state_dict(), "ncf_weights.pth")

# Step 1: Get available restaurant categories
restaurant_types = restaurants_df['simplified_category'].unique()

# Add the option for "All Categories"
restaurant_types = np.insert(restaurant_types, 0, "All Categories")

print("Available restaurant types:")
for idx, rtype in enumerate(restaurant_types, 1):
    print(f"{idx}. {rtype}")

# Step 2: User chooses a category (can choose all categories)
user_choice = int(input(f"Please select a restaurant type (1-{len(restaurant_types)}): "))

# If the user selects "All Categories"
if user_choice == 1:
    selected_type = None  # No filter for category
    print("You selected: All Categories")
else:
    selected_type = restaurant_types[user_choice - 1]  # Choose the selected category
    print(f"You selected: {selected_type}")

# Step 3: Get the user ID
user_id = input("Please enter your user ID: ")
print(f"You selected user ID: {user_id}")

# Step 4: Filter the ratings data based on user ID
user_ratings = ratings_df[ratings_df['userid'] == user_id]

# Step 5: Filter restaurants based on selected category
if selected_type is not None:  # If a category is selected
    filtered_restaurants = restaurants_df[restaurants_df['simplified_category'] == selected_type]
else:
    filtered_restaurants = restaurants_df  # No category filter, show all restaurants

# Load the NCF model if it's not already loaded (assuming it was saved as 'ncf_full_model.pth')
try:
    model  # Check if ncf_model is already defined
except NameError:
    ncf_model = torch.load("ncf_full_model.pth", map_location=torch.device('cpu'), weights_only=False)
    ncf_model.eval()  # Set the model to evaluation mode

def recommend_restaurants(user_id, model, restaurant_type, num_recommendations=10):
    # Get restaurant IDs and names
    restaurant_ids = filtered_restaurants['restid'].values
    restaurant_names = filtered_restaurants['res_name'].values  # Get corresponding restaurant names

    # Convert user_id to numerical representation using the encoder
    user_id_encoded = user_encoder.transform([user_id])[0]  # Transform user_id

    # Track the original restaurant IDs and corresponding encoded values
    restaurant_ids_encoded = restaurant_encoder.transform(restaurant_ids)  # Transform restaurant_ids
    original_restaurant_ids = restaurant_ids  # Store the original restaurant IDs

    # Predict ratings for all restaurants based on user history
    predicted_ratings = []
    for restaurant_id_enc, restaurant_name, original_rest_id in zip(restaurant_ids_encoded, restaurant_names, original_restaurant_ids):
        # Predict based on user_id and restaurant_id
        # Use the loaded NCF model (ncf_model) for prediction
        predicted_rating = model(torch.tensor([user_id_encoded]), torch.tensor([restaurant_id_enc])).item()  # Use encoded IDs
        predicted_ratings.append((restaurant_name, original_rest_id, predicted_rating))  # Store original restaurant name and ID

    # Sort restaurants based on predicted ratings
    predicted_ratings.sort(key=lambda x: x[2], reverse=True)  # Sort based on predicted rating (index 2)

    # Return top N recommendations
    return predicted_ratings[:num_recommendations]

# Get top 10 recommendations for the user
# Pass the loaded NCF model (ncf_model) to the function
recommendations = recommend_restaurants(user_id=user_id, model=model, restaurant_type=selected_type, num_recommendations=10)

# Display the recommendations with original restaurant IDs
print(f"\nTop 10 recommended restaurants for user {user_id}:")
for idx, (restaurant_name, restaurant_id, rating) in enumerate(recommendations, 1):
    print(f"{idx}. Restaurant Name: {restaurant_name}, Restaurant ID: {restaurant_id}, Predicted Rating: {rating:.2f}")

# Load the pre-trained BERT model for sentiment analysis
sent_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
sent_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')

# Step 1: Load the restaurant sentiment file (you don't seem to need this for prediction)
# sentiment_df = pd.read_csv('restaurant_sentiments.csv')  # Not necessary for prediction

# Step 2: Convert CF recommendations to DataFrame
recommendations_df = pd.DataFrame(recommendations, columns=['restaurant_name', 'restid', 'predicted_rating'])
print(recommendations[:5])  # Check the first 5 recommendations


# Step 3: Get the list of top restaurants from CF recommendations
top_restaurants = recommendations_df['restid'].tolist()

# Step 4: Extract reviews for those top restaurants from final_df
top_reviews_df = final_df[final_df['restid'].isin(top_restaurants)][['restid', 'cleaned_review']]  # Assuming 'cleaned_review' is the column with reviews

# Step 5: Define the function to get sentiment score using BERT
def get_sentiment_score(review_text):
    inputs = sent_tokenizer(review_text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = sent_model(**inputs)
        prediction = torch.argmax(outputs.logits, dim=1).item()  # 1 for POSITIVE, 0 for NEGATIVE
    return prediction

# Step 6: Apply sentiment classification to each review
top_reviews_df['predicted_sentiment'] = top_reviews_df['cleaned_review'].apply(get_sentiment_score)

# Step 7: Calculate the average sentiment for each restaurant
# Compute the mean sentiment for each restaurant
sentiment_mean = top_reviews_df.groupby('restid')['predicted_sentiment'].mean().reset_index()
sentiment_mean.columns = ['restid', 'average_sentiment']

# Step 8: Merge BERT sentiment data with CF predictions
merged_df = pd.merge(recommendations_df, sentiment_mean, on='restid', how='left')

# Step 9: Handle cold start (if average_sentiment is NaN, fill with 0 or neutral)
merged_df['average_sentiment'] = merged_df['average_sentiment'].fillna(0)

# Step 10: Compute hybrid score by combining CF predicted ratings and average sentiment
merged_df['hybrid_score'] = 0.7 * merged_df['predicted_rating'] + 0.3 * merged_df['average_sentiment']

# Step 11: Sort by hybrid score (highest to lowest)
final_recommendations = merged_df.sort_values(by='hybrid_score', ascending=False).reset_index(drop=True)

# Step 12: Display the top 10 recommended restaurants with hybrid scores
print(f"\nTop 10 personalized restaurant recommendations (CF + BERT Sentiment):")
for idx, row in final_recommendations.iterrows():
    print(f"{idx+1}. Restaurant ID: {row['restid']}, Rating: {row['predicted_rating']:.2f}, Sentiment: {row['average_sentiment']:.2f}, Hybrid Score: {row['hybrid_score']:.2f}")

# Register your ngrok account by adding your personal authentication token.
!ngrok config add-authtoken "USE YOUR OWN TOKEN"

ncf_code = """
import torch


import torch.nn as nn
class NCF(nn.Module):
    def __init__(self, num_users, num_restaurants, embedding_dim=50, hidden_layer_sizes=[128, 64]):
        super(NCF, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.restaurant_embedding = nn.Embedding(num_restaurants, embedding_dim)

        layers = []
        input_dim = embedding_dim * 2
        for size in hidden_layer_sizes:
            layers.append(nn.Linear(input_dim, size))
            layers.append(nn.ReLU())
            input_dim = size

        self.mlp = nn.Sequential(*layers)
        self.output_layer = nn.Linear(input_dim, 1)

    def forward(self, user_id, restaurant_id):
        user_embedding = self.user_embedding(user_id)
        restaurant_embedding = self.restaurant_embedding(restaurant_id)
        x = torch.cat([user_embedding, restaurant_embedding], dim=-1)
        x = self.mlp(x)
        return self.output_layer(x)


"""


with open("ncf_model_class.py", "w") as f:
    f.write(ncf_code.strip())

print("ncf_model_class.py was created!")

code = """
from fastapi import FastAPI, Request
from pydantic import BaseModel
import torch
import pandas as pd
import numpy as np
import joblib
from ncf_model_class import NCF

#prepare the app
app = FastAPI()

# load the data
ratings_df = pd.read_csv("ratings_encoded.csv")
restaurants_df = pd.read_csv("restaurants_processed.csv")
reviews_df = pd.read_csv("reviews_processed.csv")

# load encoders
user_encoder = joblib.load("user_encoder.pkl")
restaurant_encoder = joblib.load("restaurant_encoder.pkl")

# prepare the encoders for the model
n_users = len(user_encoder.classes_)
n_items = len(restaurant_encoder.classes_)

ncf_model = NCF(num_users=n_users, num_restaurants=n_items, embedding_dim=50)

ncf_model.load_state_dict(torch.load("ncf_weights.pth", map_location=torch.device("cpu")))
ncf_model.eval()


class ReviewRequest(BaseModel):
    user_id: str
    review_restid: str
    new_review: str

# --- schema ---
class RecommendationRequest(BaseModel):
    user_id: str
    category: str                  # "all" or specific category
    city: str = "all"              # NEW: "all" or one of ["Dammam","Dhahran","Jubail","Khobar"]

# --- endpoint ---
@app.post("/recommend")
async def recommend(data: RecommendationRequest):
    user_id = data.user_id
    category = data.category
    city = data.city

    if user_id not in user_encoder.classes_:
        return {"error": "User ID not found in encoder."}

    encoded_user = user_encoder.transform([user_id])[0]

    # start with all restaurants
    df = restaurants_df.copy()

    # filter by category (case-insensitive)
    if category and category.lower() != "all":
        df = df[df['simplified_category'].str.lower() == category.lower()]

    # NEW: filter by city (case-insensitive)
    if city and city.lower() != "all":
        df = df[df['city_name'].str.lower() == city.lower()]

    if df.empty:
        return {"error": "No restaurants found for the selected filters."}

    results = []
    for _, row in df.iterrows():
        rest_id = str(row["restid"])
        if rest_id not in restaurant_encoder.classes_:
            continue
        encoded_rest = restaurant_encoder.transform([rest_id])[0]

        with torch.no_grad():
            raw = ncf_model(torch.tensor([encoded_user]), torch.tensor([encoded_rest]))
            rating = 1.0 + 4.0 * torch.sigmoid(raw).item()  # keep in [1,5]

        results.append({
            "res_name": row["res_name"],
            "category": row["simplified_category"],
            "city": row["city_name"],            # NEW: include city
            "predicted_rating": round(rating, 2),
            "res_id": rest_id
        })

    results = sorted(results, key=lambda x: x["predicted_rating"], reverse=True)
    return results


# Reviews endpoint
@app.post("/review")
async def submit_review(data: ReviewRequest):
    review_restid = data.review_restid
    new_review = data.new_review

    if not review_restid.strip() or not new_review.strip():
        return {"error": "Both review_restid and new_review are required."}

    # Append to reviews_df and save
    new_row = {"restid": review_restid, "cleaned_review": new_review}
    reviews_df.loc[len(reviews_df)] = new_row
    reviews_df.to_csv("reviews_processed.csv", index=False)

    return {"success": True, "message": "Review submitted successfully!"}



"""

with open("app.py", "w") as f:
    f.write(code)

public_url = ngrok.connect(8000)
print("ðŸš€ Public URL:", public_url)

!uvicorn app:app --host 0.0.0.0 --port 8000

#!killall ngrok